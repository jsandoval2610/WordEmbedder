{"cells":[{"cell_type":"markdown","metadata":{"id":"eWGkd0ZOqb-X"},"source":["##### Installs (only run once):\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MDnxNGBNqOr5"},"outputs":[],"source":["!pip install kaggle --upgrade # for kaggle download\n","!pip install -U sentence-transformers # for SBERT pre-trained download\n","!pip install torch-geometric\n","!pip install pyg-lib torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-1.13.1+cu116.html"]},{"cell_type":"markdown","metadata":{"id":"EjBZKzkzqm8M"},"source":["##### Imports:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6L_t_4ilqiAp"},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import random\n","from random import sample\n","import pickle\n","from IPython.display import HTML, display\n","from google.colab import drive\n","import torch\n","from sentence_transformers import util\n","import gensim.downloader # pretrained word2vec and glove: https://radimrehurek.com/gensim/models/word2vec.html, https://github.com/RaRe-Technologies/gensim-data"]},{"cell_type":"markdown","metadata":{"id":"eEoPSBnRquZe"},"source":["##### Load and process a common nouns dataset from Kaggle:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1292,"status":"ok","timestamp":1675478238770,"user":{"displayName":"Liam Patty","userId":"04130295326714699617"},"user_tz":300},"id":"mIDa1fZrqyBn","outputId":"2b8b5ab0-83cd-4645-ffac-eeaf3c075b04"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading list-of-nouns.zip to /content\n","\r  0% 0.00/21.9k [00:00<?, ?B/s]\n","\r100% 21.9k/21.9k [00:00<00:00, 15.8MB/s]\n","Archive:  list-of-nouns.zip\n","  inflating: nounlist.csv            \n"]}],"source":["# downloading kaggle noun dataset - only need to run once\n","os.environ['KAGGLE_USERNAME'] = 'liamhp03'\n","os.environ['KAGGLE_KEY'] = '6797bd10159b399c7191a8303d78af30'\n","!kaggle datasets download -d leite0407/list-of-nouns # https://www.kaggle.com/datasets/leite0407/list-of-nouns\n","!unzip list-of-nouns.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZgUn6lBxrPOl"},"outputs":[],"source":["# cleaning the data\n","nouns = pd.read_csv('nounlist.csv').values.tolist()\n","for i in range(len(nouns)):\n","  nouns[i]=nouns[i][0]"]},{"cell_type":"markdown","metadata":{"id":"vccjCOJirOMU"},"source":["##### Setting up our Node class:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X4tLgNFDrcNL"},"outputs":[],"source":["class Node():\n","  def __init__(self, embedding, engl, edges, edgeweights, idx):\n","    self.emb = embedding # onehot or pre-embed encoding\n","    self.en = engl # english word\n","    self.edges = edges # list of connected edges\n","    self.edgew = edgeweights # edgeweights corresponding to edges\n","    self.idx = idx # node index"]},{"cell_type":"markdown","metadata":{"id":"Rak0Xptorgqj"},"source":["##### Building a graph:"]},{"cell_type":"markdown","metadata":{"id":"0v7z2FEnrral"},"source":["Loading a pretrained model to select edges:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":706039,"status":"ok","timestamp":1675478944801,"user":{"displayName":"Liam Patty","userId":"04130295326714699617"},"user_tz":300},"id":"V1vQxoMDrjlH","outputId":"88d26f41-09fe-422d-f02c-31a99a5161a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["[=================================================-] 99.9% 1661.2/1662.8MB downloaded\n"]}],"source":["word2vec = gensim.downloader.load(\"word2vec-google-news-300\") # load a pretrained word2vec model"]},{"cell_type":"markdown","metadata":{"id":"1M9wEYO-r9nb"},"source":["##### Test the pretrained model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1675478944802,"user":{"displayName":"Liam Patty","userId":"04130295326714699617"},"user_tz":300},"id":"btnQmyGnr8Pf","outputId":"f43ebe5a-2ce0-4329-f052-572a761cd6e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: word2vec\n","Word: tramp\n","Embedding shape: (300,)\n"]}],"source":["word = nouns[random.randint(0, len(nouns)-1)] # choose a random word from our words dataset\n","print(\"Model: word2vec\")\n","print(\"Word: \" + word)\n","print(\"Embedding shape: \" + str(word2vec[word].shape))"]},{"cell_type":"markdown","metadata":{"id":"beBJNXv7ryPc"},"source":["##### Define a onehot encoding function:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aGd82Czur1PY"},"outputs":[],"source":["def onehot(word):\n","  v = torch.zeros(len(nouns))\n","  v[nouns.index(word)] = 1\n","  return v"]},{"cell_type":"markdown","metadata":{"id":"QQCg3ljer0nf"},"source":["##### Create a graph of noun nodes:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5gyXg8k7sZb-"},"outputs":[],"source":["strToNode = {} # dictionary that maps the english words to their nodes"]},{"cell_type":"markdown","metadata":{"id":"EWbrB6Bcs3O5"},"source":["##### Quick function to visualizing model progress:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OJWSZidjs18A"},"outputs":[],"source":["# src: https://stackoverflow.com/questions/46939393/how-do-i-use-updatable-displays-on-colab\n","def progressbar(value, max=100):\n","    return HTML(\"\"\"\n","        <progress\n","            value='{value}'\n","            max='{max}',\n","            style='width: 80%'\n","        >\n","            {value}\n","        </progress>\n","    \"\"\".format(value=value, max=max))"]},{"cell_type":"markdown","metadata":{"id":"kIeRSLlStXw4"},"source":["##### Initialize each node and add it to the dictionary"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":34},"id":"nWSUC8VwtKj0","outputId":"fb19bc0a-1016-48ca-c9c1-ad2c4851ed00"},"outputs":[{"data":{"text/html":["\n","        <progress\n","            value='6800'\n","            max='6800',\n","            style='width: 80%'\n","        >\n","            6800\n","        </progress>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["progress = display(progressbar(0, len(nouns)), display_id=True)\n","for w in range(len(nouns)):\n","  cword = nouns[w] # current word\n","  sim_words = [] # edges\n","  sim_vals = [] # edge weights\n","\n","  outofvocab=0 # ignore words not in the word2vec vocab\n","  for i in range(len(nouns)):\n","    pword = nouns[i] # potential edge\n","    if(pword!=cword): # make sure they aren't the same word\n","      try:\n","        sim = word2vec.similarity(cword, pword)\n","        if(sim > .45): # iterate through every other word, if its similarity is above the threshold then add it as an edge with the similarity as the weight\n","          sim_words.append(pword)\n","          sim_vals.append(sim)\n","      except:\n","        outofvocab+=1\n","    \n","  # create the node and add it to our graph\n","  if(outofvocab < 300): # check to make sure cword is in vocab\n","    wordnode = Node(word2vec[cword], cword, sim_words, sim_vals, w)\n","    strToNode.update({cword: wordnode})\n","\n","  # update our progress bar\n","  if(w%50 == 0):\n","    progress.update(progressbar(w, len(nouns)))\n","progress.update(progressbar(len(nouns), len(nouns)))"]},{"cell_type":"markdown","metadata":{"id":"7xBCQS15wrO6"},"source":["Testing the graph:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BFdLEvsQwOHd"},"outputs":[],"source":["word = 'apple'\n","print(\"Word: \"+word)\n","print(\"Embedding Shape: \" + str(strToNode[word].emb.shape))\n","print(\"Edges: \" + strToNode[word].edges)\n","print(\"Edgeweights: \" + strToNode[word].edgew)"]},{"cell_type":"markdown","metadata":{"id":"dpk3Tn0luFh0"},"source":["##### Save our graph to google drive:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16695,"status":"ok","timestamp":1675309533649,"user":{"displayName":"Julian Sandoval","userId":"18319358280031138217"},"user_tz":300},"id":"jERykOoyuFNB","outputId":"d1d30af2-4b65-4bc4-d890-4301e0d4314c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YodZSuRDvBOb"},"outputs":[],"source":["def Save(graph):\n","    writename = \"init-graph-w2vec-.35thresh\"\n","    with open(\"/content/drive/My Drive/J-Term 2023/input-graphs/\"+writename+\".txt\", \"wb\") as pkl_handle:\n","        pickle.dump(graph, pkl_handle)\n","Save(strToNode)\n","del(strToNode)"]},{"cell_type":"markdown","metadata":{"id":"vwR78oD4vNNl"},"source":["##### Print all saved graphs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":187},"executionInfo":{"elapsed":146,"status":"error","timestamp":1675309647435,"user":{"displayName":"Julian Sandoval","userId":"18319358280031138217"},"user_tz":300},"id":"xf6YMpnlvMu2","outputId":"679f957f-c02a-47ac-c356-c202415464b8"},"outputs":[{"ename":"FileNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-53202ff2a5ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/J-Term 2023/input-graphs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\": \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msaved\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/J-Term 2023/input-graphs'"]}],"source":["saved = os.listdir(\"/content/drive/My Drive/J-Term 2023/input-graphs\")\n","t=[print(str(i)+\": \"+saved[i]) for i in range(len(saved))]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SpRwKxDhNYHe"},"outputs":[],"source":["def Load(graphname):\n","    with open(\"/content/drive/My Drive/J-Term 2023/input-graphs/\" + graphname, \"rb\") as pkl_handle:\n","        output = pickle.load(pkl_handle)\n","        print(\"loaded: \"+graphname)\n","        return output\n","\n","loaded_graph = Load(saved[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bbOkmhrYKv4W"},"outputs":[],"source":["import matplotlib.pyplot as plt \n","\n","plt.figure(figsize=(20,10))\n","plotted = {} # idx: (xcor, ycor)\n","for i in range(0, 10):\n","  x = random.randint(0,20)\n","  y = random.randint(0,20)\n","  plotted.update({i: [x,y]})\n","  plt.plot(x,y, marker=\"bo\")\n","  for e in strToNode[strToNode[i]].edges:\n","    if(e.idx < i):\n","      plt.plot([x, plotted[e.idx][0]], [y, plotted[e.idx][1]])\n","\n","plt.show()"]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}